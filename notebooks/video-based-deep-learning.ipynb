{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu0SPRuCCCi3"
   },
   "source": [
    "# Similarity Modeling 1/2: Recognizing Pigs with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEXDPoGdCCi5"
   },
   "source": [
    "## Student\n",
    "Gallus Huber (51905700)\n",
    "\n",
    "## Timesheet\n",
    "2022-10-16 09:00 - 12:30 SIM 1 & 2 <br />\n",
    "2022-10-16 15:00 - 17:00 SIM 3 <br />\n",
    "2022-10-18 10:00 - 13:00 SIM 4 & 5 <br />\n",
    "2022-10-27 20:00 - 22:00 SIM 6 <br />\n",
    "2022-10-28 10:00 - 14:00 SIM 7 & 8 & 9 <br />\n",
    "2022-11-02 08:00 - 14:00 Writing abstracts <br />\n",
    "2022-11-10 14:00 - 15:00 Meeting <br />\n",
    "2022-11-24 16:00 - 17:30 Setup <br />\n",
    "2022-11-26 09:00 - 12:00 Implementation data preparation <br />\n",
    "2022-11-27 10:00 - 10:30 Meeting <br />\n",
    "2022-11-27 10:30 - 15:00 Implementation model <br />\n",
    "2022-12-01 16:00 - 20:00 Training of model <br />\n",
    "2022-12-03 14:00 - 18:00 Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZmOtdK2CCi6"
   },
   "source": [
    "## Required libraries and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6p8Iq4RPCCi8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670086975207,
     "user_tz": -60,
     "elapsed": 8874,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "91261f33-6079-4b6a-c611-5dcd3ac1eef0"
   },
   "outputs": [],
   "source": [
    "!cat requirements-dl-video.txt\n",
    "!pip install -r requirements-dl-video.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EChKkugZCCi9"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QORN-wo0CCi-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670086987008,
     "user_tz": -60,
     "elapsed": 7749,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pathlib\n",
    "import collections\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2 as cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "import skimage.transform as trans\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.applications as appl\n",
    "import keras.models as models\n",
    "import keras.layers as layers\n",
    "import keras.utils as utils\n",
    "import keras.callbacks as cb\n",
    "import keras.optimizers as optimizers\n",
    "\n",
    "import visualkeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSaXtxqNCCi-"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zK-Kl6yxCCjA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670087251243,
     "user_tz": -60,
     "elapsed": 226,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "outputs": [],
   "source": [
    "# path to ground truth and videos\n",
    "DATA_PATH = \"/content/drive/MyDrive/data/\"\n",
    "GROUND_TRUTH_PATH = os.path.join(DATA_PATH, \"sim12-ground-truth-muppets\")\n",
    "VIDEO_PATH = os.path.join(DATA_PATH, \"video\")\n",
    "\n",
    "# path to train data\n",
    "TRAINING_PATH = os.path.join(VIDEO_PATH, 'training_data')\n",
    "VALIDATION_PATH = os.path.join(VIDEO_PATH, 'validation_data')\n",
    "TESTING_PATH = os.path.join(VIDEO_PATH, 'testing_data')\n",
    "\n",
    "# path to save and load trained models\n",
    "MODEL_PATH = \"/content/drive/MyDrive/data/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Project structure"
   ],
   "metadata": {
    "collapsed": false,
    "id": "6yBM5yfrVtgu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create dirs for training\n",
    "train_path_0 = os.path.join(TRAINING_PATH, '0')\n",
    "train_path_1 = os.path.join(TRAINING_PATH, '1')\n",
    "\n",
    "# create dirs for validation\n",
    "valid_path_0 = os.path.join(VALIDATION_PATH, '0')\n",
    "valid_path_1 = os.path.join(VALIDATION_PATH, '1')\n",
    "\n",
    "# create dirs for testing\n",
    "test_path_0 = os.path.join(TESTING_PATH, '0')\n",
    "test_path_1 = os.path.join(TESTING_PATH, '1')"
   ],
   "metadata": {
    "id": "kWOdqkGYVtgv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670087024513,
     "user_tz": -60,
     "elapsed": 200,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create dirs for training\n",
    "if not os.path.exists(TRAINING_PATH):\n",
    "    os.makedirs(TRAINING_PATH)\n",
    "if not os.path.exists(train_path_0):\n",
    "    os.makedirs(train_path_0)\n",
    "if not os.path.exists(train_path_1):\n",
    "    os.makedirs(train_path_1)\n",
    "\n",
    "# create dirs for validation\n",
    "if not os.path.exists(VALIDATION_PATH):\n",
    "    os.makedirs(VALIDATION_PATH)\n",
    "if not os.path.exists(valid_path_0):\n",
    "    os.makedirs(valid_path_0)\n",
    "if not os.path.exists(valid_path_1):\n",
    "    os.makedirs(valid_path_1)\n",
    "\n",
    "# create dirs for testing\n",
    "if not os.path.exists(TESTING_PATH):\n",
    "    os.makedirs(TESTING_PATH)\n",
    "if not os.path.exists(test_path_0):\n",
    "    os.makedirs(test_path_0)\n",
    "if not os.path.exists(test_path_1):\n",
    "    os.makedirs(test_path_1)\n",
    "\n",
    "# create dir for models\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)"
   ],
   "metadata": {
    "id": "bspF6MlcVtgw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Data Preparation"
   ],
   "metadata": {
    "collapsed": false,
    "id": "RYHKH5dNVtgx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 List all videos"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Pa1UKEt4Vtgx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "videos = glob.glob(VIDEO_PATH + '/*.avi')\n",
    "\n",
    "videos_tmp = []\n",
    "for video in videos:\n",
    "    videos_tmp.append(pathlib.Path(video).name)\n",
    "videos = videos_tmp\n",
    "\n",
    "videos"
   ],
   "metadata": {
    "id": "SZpwqEUdVtgx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Load ground truth"
   ],
   "metadata": {
    "collapsed": false,
    "id": "P760GHefVtgx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2.1 Read ground truth from CSV"
   ],
   "metadata": {
    "collapsed": false,
    "id": "JAKfonUSVtgx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ground_truth = dict()\n",
    "\n",
    "# read in corresponding ground truth\n",
    "for video in videos:\n",
    "        file_name = glob.glob(GROUND_TRUTH_PATH + '/*' + os.path.splitext(video)[0] + '*.csv')\n",
    "        csv = pd.read_csv(os.path.join(file_name[0]), sep=\";\")\n",
    "        ground_truth[video] = csv"
   ],
   "metadata": {
    "id": "T3b9LRm9Vtgz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2.2 Plot the ground truth"
   ],
   "metadata": {
    "collapsed": false,
    "id": "YBjfaXTuVtgz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_gt(gt, column):\n",
    "    plot = plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # define subplot\n",
    "    subplot = plot.add_subplot(111)\n",
    "    subplot.set_title(f\"Occurrences in Frames\")\n",
    "\n",
    "    # define x axis\n",
    "    subplot.set_xlabel('Frames')\n",
    "\n",
    "    # mark occurrences\n",
    "    frame_indicator = np.zeros(len(gt['Frame_number']))\n",
    "    frame_indicator[gt[gt[column] == 1].index] = 1\n",
    "\n",
    "    subplot.plot(gt[column].index, frame_indicator)\n",
    "    plt.show()\n",
    "\n",
    "for video in videos:\n",
    "    plot_gt(ground_truth[video], \"Pigs\")"
   ],
   "metadata": {
    "id": "wuTQHyDCVtgz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Load video data"
   ],
   "metadata": {
    "collapsed": false,
    "id": "OZvzfqVpVtg0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.1 Data structure to store splits"
   ],
   "metadata": {
    "collapsed": false,
    "id": "wo0D7DObVtg0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = dict()\n",
    "valid_data = dict()\n",
    "test_data = dict()"
   ],
   "metadata": {
    "id": "qjIGHMqPVtg0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.2 Create splits\n",
    "The ground truth is split into training set, validation set and testing set."
   ],
   "metadata": {
    "collapsed": false,
    "id": "rdeGylIiVtg0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for video in videos:\n",
    "    train_data[video], test_data[video] = ms.train_test_split(ground_truth[video], test_size=0.2, shuffle=True, random_state=42)\n",
    "    train_data[video], valid_data[video] = ms.train_test_split(train_data[video], test_size=0.25, random_state=42)"
   ],
   "metadata": {
    "id": "ue7NHQmTVtg1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.3 Get frames of video\n",
    "The frames are written to the corresponding directory based on the splits and ground truth."
   ],
   "metadata": {
    "collapsed": false,
    "id": "J2xC8JZOVtg1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvnX8mhZCCjE"
   },
   "outputs": [],
   "source": [
    "def retrieve_frames(video_file):\n",
    "    # open a videoCap for this video\n",
    "    video_cap = cv2.VideoCapture(os.path.join(VIDEO_PATH, video_file))\n",
    "\n",
    "    # used to identify frame in ground truth\n",
    "    frame_nr = 0\n",
    "\n",
    "    # read all frames of video\n",
    "    while True:\n",
    "        # get one frame\n",
    "        success_read, frame_read = video_cap.read()\n",
    "\n",
    "        # no more frames available\n",
    "        if not success_read:\n",
    "            break\n",
    "\n",
    "        # create filename\n",
    "        filename = 'frame_%d.jpg' % frame_nr\n",
    "\n",
    "        # write frames to dir\n",
    "        if frame_nr in train_data[video_file].Frame_number:\n",
    "            if (train_data[video_file][train_data[video_file].Frame_number == frame_nr].Pigs == 0).bool():\n",
    "                cv2.imwrite(os.path.join(train_path_0, filename), frame_read)\n",
    "            else:\n",
    "                cv2.imwrite(os.path.join(train_path_1, filename), frame_read)\n",
    "\n",
    "        elif frame_nr in valid_data[video_file].Frame_number:\n",
    "            if (valid_data[video_file][valid_data[video_file].Frame_number == frame_nr].Pigs == 0).bool():\n",
    "                cv2.imwrite(os.path.join(valid_path_0, filename), frame_read)\n",
    "            else:\n",
    "                cv2.imwrite(os.path.join(valid_path_1, filename), frame_read)\n",
    "\n",
    "        elif frame_nr in test_data[video_file].Frame_number:\n",
    "            if (test_data[video_file][test_data[video_file].Frame_number == frame_nr].Pigs == 0).bool():\n",
    "                cv2.imwrite(os.path.join(test_path_0, filename), frame_read)\n",
    "            else:\n",
    "                cv2.imwrite(os.path.join(test_path_1, filename), frame_read)\n",
    "\n",
    "        # go to next frame\n",
    "        frame_nr += 1\n",
    "\n",
    "\n",
    "for video in videos:\n",
    "    retrieve_frames(video)\n",
    "    print(f\"Loaded frames of {video} successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6LjAkswCCjE"
   },
   "source": [
    "# 2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Build model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "_k54XZ9DVtg2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.1 Create the model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "AqANRclSVtg2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPqd53OPCCjH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670087003717,
     "user_tz": -60,
     "elapsed": 3518,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "837608ec-266e-4ba1-d493-f8449a10b7f5"
   },
   "outputs": [],
   "source": [
    "# create base model -> pretrained model\n",
    "base_model = appl.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# add base model\n",
    "head_model = base_model.output\n",
    "\n",
    "# additional hidden layer\n",
    "head_model = layers.Flatten()(head_model)\n",
    "head_model = layers.Dense(units=4096,activation=\"relu\")(head_model)\n",
    "head_model = layers.Dense(units=4096,activation=\"relu\")(head_model)\n",
    "\n",
    "# create output layer\n",
    "head_model = layers.Dense(1, activation=\"softmax\")(head_model)\n",
    "\n",
    "# create model\n",
    "model = models.Model(base_model.input, head_model)\n",
    "\n",
    "# base model should not be trained\n",
    "for layer in base_model.layers:\n",
    "\tlayer.trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.2 Plot the model\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "_QzyLzerVtg3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set colors for each used layer\n",
    "color_map = collections.defaultdict(dict)\n",
    "color_map[layers.InputLayer]['fill'] = '#7D670C'\n",
    "color_map[layers.Conv2D]['fill'] = '#060A7D'\n",
    "color_map[layers.MaxPooling2D]['fill'] = '#7C0400'\n",
    "color_map[layers.Flatten]['fill'] = '#7D066F'\n",
    "color_map[layers.Dense]['fill'] = '#0C7D34'\n",
    "\n",
    "# plot the model\n",
    "visualkeras.layered_view(model, legend=True, color_map=color_map)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "id": "AVw0ddhMVtg3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670087008164,
     "user_tz": -60,
     "elapsed": 568,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "293b8bb4-1157-4696-ac2b-e3a8f55e68e2"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfvb2uGLCCjI"
   },
   "source": [
    "## 2.2 Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACR_Bii3CCjI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670087011640,
     "user_tz": -60,
     "elapsed": 210,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(learning_rate=1e-4, momentum=0.9, decay=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIdnKb3_CCjI"
   },
   "source": [
    "## 2.3 Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.1 Train model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "qu9KS168Vtg4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load training set\n",
    "train_ds = utils.image_dataset_from_directory(\n",
    "    directory=TRAINING_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    batch_size=32,\n",
    "    image_size=(224, 224))\n",
    "\n",
    "# load validation set\n",
    "validation_ds = utils.image_dataset_from_directory(\n",
    "    directory=VALIDATION_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    batch_size=32,\n",
    "    image_size=(224, 224))\n",
    "\n",
    "# callback to avoid overfitting and storing best model\n",
    "callbacks = [cb.EarlyStopping(monitor='val_loss', patience=2),\n",
    "             cb.ModelCheckpoint(filepath=os.path.join(MODEL_PATH, 'best_model.h5'), monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "# train model\n",
    "with tf.device('/device:GPU:0'):\n",
    "  history = model.fit(train_ds, epochs=25, validation_data=validation_ds, callbacks=callbacks)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "UsnA5nzEVtg4",
    "executionInfo": {
     "status": "error",
     "timestamp": 1670087762843,
     "user_tz": -60,
     "elapsed": 266669,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "a658e2cb-924f-425c-dad0-22a412f528f2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.3 Plot training results"
   ],
   "metadata": {
    "collapsed": false,
    "id": "kVBoLbuHVtg4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss [CrossEntropy]')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.ylim([0, 100])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy [%]')"
   ],
   "metadata": {
    "id": "Dkbfe2-tVtg5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Results"
   ],
   "metadata": {
    "collapsed": false,
    "id": "YNW2b20LVtg6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Load best model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "P2sVL1vZ7JE1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = models.load_model(os.path.join(MODEL_PATH, 'best_model.h5'))"
   ],
   "metadata": {
    "id": "KGayZYWp7JE1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Evaluate model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "8Z9fMF067JE1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load testing set\n",
    "test_ds = utils.image_dataset_from_directory(\n",
    "    directory=TESTING_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    batch_size=32,\n",
    "    image_size=(224, 224))\n",
    "\n",
    "# evaluate the model\n",
    "results = model.evaluate(test_ds, batch_size=128)\n",
    "\n",
    "results"
   ],
   "metadata": {
    "id": "m8qysU8K7JE2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Prediction for videos"
   ],
   "metadata": {
    "collapsed": false,
    "id": "LESnjk6yVtg6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "video_prediction = dict()\n",
    "\n",
    "for video in videos:\n",
    "    video_prediction[video] = []"
   ],
   "metadata": {
    "id": "MTa0B-LEVtg7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for video in videos:\n",
    "    # open videoCap for video\n",
    "    video_capture = cv2.VideoCapture(os.path.join(VIDEO_PATH, video))\n",
    "\n",
    "    # read all frames of video\n",
    "    while True:\n",
    "        # get one frame\n",
    "        success, frame = video_capture.read()\n",
    "\n",
    "        # identify frame\n",
    "        frame_number = 0\n",
    "\n",
    "        # no more frames available\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # reshape frame to fit to input layer\n",
    "        frame = trans.resize(frame, output_shape=(224, 224))\n",
    "        frame = np.reshape(frame,[1, 224, 224, 3])\n",
    "\n",
    "        # use model to make prediction for frame\n",
    "        prediction = model.predict(frame)\n",
    "        print((frame_number, prediction))\n",
    "        video_prediction[video].append((frame_number, prediction))\n",
    "\n",
    "video_prediction"
   ],
   "metadata": {
    "id": "5sa-iIocVtg7"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sm-dl-video",
   "language": "python",
   "name": "sm-dl-video"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/mjhof/similarity-modeling/blob/setup-video-dl/notebooks/video-based-deep-learning.ipynb",
     "timestamp": 1670087770179
    },
    {
     "file_id": "https://github.com/mjhof/similarity-modeling/blob/setup-video-dl/notebooks/video-based-deep-learning.ipynb",
     "timestamp": 1670085701232
    }
   ]
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
